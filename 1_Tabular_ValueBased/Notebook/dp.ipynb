{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DP\n",
    "动态规划方法包括基于模型的策略迭代和值迭代，两个方法都基于值函数进行策略评估和策略改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Policy Iteration\n",
    "定义类`PolicyIter`，并读取环境中状态空间、动作空间、状态转移等信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "class PolicyIter():\n",
    "    def __init__(self, env, gamma = 0.8, error = 0.00001, max_iter = 1000) -> None:\n",
    "        self.env = env\n",
    "        self.states = env.states\n",
    "        self.actions = env.actions\n",
    "        self.terminal_rewards = env.terminal_rewards\n",
    "        self.states_actions = env.states_actions\n",
    "\n",
    "        self.v = dict()\n",
    "        for state in self.states:\n",
    "            if state not in self.terminal_rewards:\n",
    "                self.v[state] = 0.0\n",
    "            else:\n",
    "                self.v[state] = self.terminal_rewards[state]\n",
    "        self.pi = dict()\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.error = error\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def policy_init(self):\n",
    "        for state in self.states:\n",
    "            if state in self.terminal_rewards:\n",
    "                self.pi[state] = None\n",
    "            else:\n",
    "                self.pi[state] = random.sample(list(self.states_actions[state]), 1)[0]\n",
    "\n",
    "    def policy_evaluate(self):\n",
    "        for _ in range(self.max_iter):\n",
    "            delta = 0.0\n",
    "            for state in self.states:\n",
    "                if state not in self.terminal_rewards:\n",
    "                    self.env.reset()\n",
    "                    action = self.pi[state]\n",
    "                    self.env.set_state(state)\n",
    "                    next_state, reward, is_terminal, info = self.env.step(action)\n",
    "                    self.env.reset()\n",
    "                    v_last = self.v[state]\n",
    "                    self.v[state] = reward + self.gamma*self.v[next_state]\n",
    "                    delta = max(delta, abs(self.v[state] - v_last))\n",
    "            if delta <= self.error:\n",
    "                break\n",
    "\n",
    "    def policy_improve(self):\n",
    "        policy_stable = True\n",
    "        for state in self.states:\n",
    "            if state not in self.terminal_rewards:\n",
    "                max_action = list(self.states_actions[state])[0]\n",
    "                max_q = -999999.0\n",
    "                for action in list(self.states_actions[state]):\n",
    "                    self.env.reset()\n",
    "                    self.env.set_state(state)\n",
    "                    next_state, reward, is_terminal, info = self.env.step(action)\n",
    "                    self.env.reset()\n",
    "                    q_value = reward + self.gamma*self.v[next_state]\n",
    "                    if q_value > max_q:\n",
    "                        max_action = action\n",
    "                        max_q = q_value\n",
    "                if self.pi[state] != max_action:\n",
    "                    policy_stable = False\n",
    "                    self.pi[state] = max_action\n",
    "        return policy_stable\n",
    "\n",
    "    def forward(self, iter):\n",
    "        self.env.reset()\n",
    "        for _ in range(int(self.max_iter/50)):\n",
    "            env.render()\n",
    "            if _ == 0:\n",
    "                print(\"iteration:{}, init_state: {}\".format(iter, self.env.state))\n",
    "            next_state, reward, is_terminal, info = self.env.step(self.pi[self.env.state])\n",
    "            print(\"iteration:{}, next_state:{}, reward:{}, is_terminal:{}\".format(iter, next_state, reward, is_terminal))\n",
    "    \n",
    "    def policy_iterate(self):\n",
    "        self.policy_init()\n",
    "        count = 0\n",
    "        for _ in range(self.max_iter):\n",
    "            # self.forward(_)\n",
    "            self.policy_evaluate()\n",
    "            policy_stable = self.policy_improve()\n",
    "            if policy_stable == True:\n",
    "                count += 1\n",
    "            if count >= 10:\n",
    "                print(\"Policy now stabled.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('GridWorld-v0')\n",
    "PI = PolicyIter(env)\n",
    "PI.policy_iterate()\n",
    "for _ in range(10):\n",
    "    PI.forward(\"Test\")\n",
    "print()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Value Iteration\n",
    "`ValueIter`和`PolicyIter`算法逻辑基本一样，只是值迭代时，目的是对状态的值函数进行迭代，所以在策略评估时只进行一次，就更新策略，直到值基本不变时停止循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIter():\n",
    "    def __init__(self, env, gamma = 0.8, error = 0.00001, max_iter = 1000) -> None:\n",
    "        self.env = env\n",
    "        self.states = env.states\n",
    "        self.actions = env.actions\n",
    "        self.terminal_rewards = env.terminal_rewards\n",
    "        self.states_actions = env.states_actions\n",
    "\n",
    "        self.v = dict()\n",
    "        for state in self.states:\n",
    "            if state not in self.terminal_rewards:\n",
    "                self.v[state] = 0.0\n",
    "            else:\n",
    "                self.v[state] = self.terminal_rewards[state]\n",
    "        self.pi = dict()\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.error = error\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def policy_init(self):\n",
    "        for state in self.states:\n",
    "            if state in self.terminal_rewards:\n",
    "                self.pi[state] = None\n",
    "            else:\n",
    "                self.pi[state] = random.sample(list(self.states_actions[state]), 1)[0]\n",
    "\n",
    "    def policy_evaluate(self): # 1 \n",
    "        delta = 0.0\n",
    "        values_stable = True\n",
    "        for state in self.states:\n",
    "            if state not in self.terminal_rewards:\n",
    "                self.env.reset()\n",
    "                action = self.pi[state]\n",
    "                self.env.set_state(state)\n",
    "                next_state, reward, is_terminal, info = self.env.step(action)\n",
    "                self.env.reset()\n",
    "                v_last = self.v[state]\n",
    "                self.v[state] = reward + self.gamma*self.v[next_state]\n",
    "                delta = max(delta, abs(self.v[state] - v_last))\n",
    "        if delta >= self.error:\n",
    "            values_stable = False\n",
    "        return values_stable\n",
    "\n",
    "    def policy_improve(self):\n",
    "        for state in self.states:\n",
    "            if state not in self.terminal_rewards:\n",
    "                max_action = list(self.states_actions[state])[0]\n",
    "                max_q = -999999.0\n",
    "                for action in list(self.states_actions[state]):\n",
    "                    self.env.reset()\n",
    "                    self.env.set_state(state)\n",
    "                    next_state, reward, is_terminal, info = self.env.step(action)\n",
    "                    self.env.reset()\n",
    "                    q_value = reward + self.gamma*self.v[next_state]\n",
    "                    if q_value > max_q:\n",
    "                        max_action = action\n",
    "                        max_q = q_value\n",
    "                self.pi[state] = max_action\n",
    "\n",
    "    def forward(self, iter):\n",
    "        self.env.reset()\n",
    "        for _ in range(int(self.max_iter/50)):\n",
    "            env.render()\n",
    "            if _ == 0:\n",
    "                print(\"iteration:{}, init_state: {}\".format(iter, self.env.state))\n",
    "            next_state, reward, is_terminal, info = self.env.step(self.pi[self.env.state])\n",
    "            print(\"iteration:{}, next_state:{}, reward:{}, is_terminal:{}\".format(iter, next_state, reward, is_terminal))\n",
    "    \n",
    "    def policy_iterate(self):\n",
    "        self.policy_init()\n",
    "        count = 0\n",
    "        for _ in range(self.max_iter):\n",
    "            # self.forward(_)\n",
    "            values_stable = self.policy_evaluate()\n",
    "            if values_stable == True:\n",
    "                count += 1\n",
    "            if count >= 10:\n",
    "                print(\"Value now stabled.\")\n",
    "                break\n",
    "            self.policy_improve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('GridWorld-v0')\n",
    "VI = ValueIter(env)\n",
    "VI.policy_iterate()\n",
    "for _ in range(10):\n",
    "    VI.forward(\"Test\")\n",
    "print()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
